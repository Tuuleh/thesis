INSTRUMENTS

All of the instruments were written in JavaScript on top of the Node.js runtime environment, with Express web-framework, hosted on a Digital Oceans server in San Francisco. The spatial working memory task was written as a stand-alone application, and modified plugins of the jsPsych-library (de Leeuw, 2014) were used for the other experiments. All of the experiments, the plugins, the back end structure and descriptions of the dependencies for the experiment can be found in Pöllänen (2014). The participants were asked to use a mouse, but in the absence of one, all of the tasks could be completed with a trackpad. Access with mobile devices was blocked to all parts of the experiment to make sure that data was comparable across participants.


INVENTORIES

GEQ

The Group Environment Questionnaire (GEQ) was chosen as a measure of group cohesion because it is well established with a long history of use in sports psychology and group research, with recent evidence suggesting adequate multilevel factorial validity (Fletcher & Whitton, 2014). An instrument from sports psychology was preferred over those from organizational psychology, as interviews with players indicated that the items meant for athletic teams were considered more relevant to their gameplay experience. The players' satisfaction with the instrument drove the decision of not developing a new instrument for the purpose, but rather attempting to repurpose GEQ. The items were modified slightly to match the context of virtual games (e.g. using the word "players" instead of "athletes"), and the modified items can be found in Appendix 1. The goodness-of-fit of a sports psychology inventory for the purpose of this study is something that should be taken into account in interpreting the results of this study. 

GEQ is a general, rather than situation-specific measure of cohesion in sports teams. It consists of the following four sub-scales forming a four-factor model of cohesion (Brawley, Carron, & Widmeyer, 1987):

Group Integration–Social (GI-S) conceptualizes a team member's assessment of the group's closeness, similarity and bonding as a social unit - for example, “members of our team do not stick together outside of practices and games”.

Group Integration–Task (GI-T) conceptualizes the member's assessment of the group's closeness, similarity and bonding around the group's task - for example, “our team is united in trying to reach its goals for performance”.

Individual Attractions to the Group–Social (ATG-S) conceptualizes to the member's notions of the social interactions and personal acceptance within the team, for example “some of my best friends are on this team”. 

Individual Attractions to the Group–Task (ATG-T) conceptualizes a member's feelings about personal involvement related to the group's common goals and productivity - for instance, “I do not like the style of play on this team”. 


----------------
My analyses did not support the four-factor structure in the original inventory, so I searched for optional solutions with different factor analytic procedures. An exploration through parallel analysis, optimal coordination and acceleration factor on R suggested between one and three factors. A comparison between different solutions netted very similar results, with two strong factors - a task-related factor that subsumed items from ATGT and GIT, and a slightly weaker socially oriented factor that subsumed ATGS and GIS. The exceptions were items 2 and 11 - 2 did not have a good loading on either component, and 11 loaded on the task-related component, despite of originally being an item on the social scale. Item number two sounds "I'm not happy with the play time I get", and it could simply be that it lacks validity for the e-sports context, where playrs do not get benched the same way as they do in traditional sports. In a casual context, players get plenty of play time, since the team cannot engage in games without them being present. Item number 11 is the following: "Members of our team would rather go out on their own than get together as a team." In the original inventory, this refers to socialization with the team. In an online games context, this item is easier to interpret so that the individual team members rather play solo games than queue together with the team, making it a task-related item.

A promax-rotated two-factor solution with the following characteristics:
----------------

TLX

The NASA Task Load Inventory (TLX) was selected as a well-established, short task load inventory with good metric characteristics (Hart & Staveland, 1988; Hart, 2006). The instrument originally consisted of two parts. In the first part, six sub-scales are presented on a single page, with the following description of each of the scale:

    Mental Demand: How much mental and perceptual activity was required? Was the task easy or demanding, simple or complex?
    Physical Demand: How much physical activity was required? Was the task easy or demanding, slack or strenuous?
    Temporal Demand: How much time pressure did you feel due to the pace at which the tasks or task elements occurred? Was the pace slow or rapid?
    Performance: How successful were you in performing the task? How satisfied were you with your performance?
    Frustration: How irritated, stressed, and annoyed versus content, relaxed, and complacent did you feel during the task?
    Effort: How hard did you have to work (mentally and physically) to accomplish your level of performance?

The items are rated on a 100-points range with 5-points increments. In the original version, the second part of the inventory creates individual weighing by importance for each of the six subscales by prompting the subjects to compare the categories pairwise based on their perceived importance for the task load. The estimated task loads are then weighed according to their importance. The version used for this study, however, consists of only the first part of the original test, without the pairwise comparisons. This modification was done to simplify the study design, with evidence indicating that this procedure (referred to as the Raw TLX) is as sensitive as the original instrument (Hart, 2006).


COGNITIVE TASKS

A major difficulty in selecting instruments for the cognitive tasks was a lack of prior literature on the topic of differential performance in expert video-game players. I used previous literature on video game training effects and literature comparing expert gamers with novices to guide my decision-making, and decided to program four cognitive tasks: the Eriksen flanker task, a mental rotation task, a spatial span task, and the Tower of London. 

Flanker task

Two conflict-tasks have previously been used in game studies: varying versions of the Eriksen flanker task (Eriksen & Eriksen, 1974) and the Stroop-task. The Stroop task has been used before by e.g. Maillot, Perrot and Hartley (2012); and Anderson, Bailey and West (2012). Maillot et al. (2012) found that older adults in their study improved significantly in their performance on resisting the interference following video game play. Anderson et al. (2010) compared players with different levels of experience and discovered no difference between less experienced and more experienced gamers in the Stroop interference effect, but did find a negative association between video game experience and proactive cognitive control. 

Since my participant pool is international, I decided for the flanker task, as the fact that the participants are not all native speakers of English could influence the amount of interference in the Stroop task. A variation of the flanker task -- Attentional Network Test, or ANT -- was used by Dye, Green and Bavelier (2009), who discovered that action video game players made faster correct responses to incongruent trials. Best (2012) used a children's version of the ANT to discover that exergaming enhances children's speed in resolving interference from conflicting visuospatial stimuli. The Flanker task used for this experiment was programmed roughly in a similar manner to the Flanker task in the PEBL library (Mueller & Piper, 2014), which was based on Stins, Polderman, Boomsma and deGeus (2007). During the task, the participant is presented with images of five arrows in a row. Their task is to press the arrow key corresponding to the direction of the middle arrow, which points either to the same direction as the arrows flanking it (a congruent trial) or to the opposite direction (incongruent trial). 
The measures gained on the Flanker task are the speed and accuracy in responding to congruent vs. incongruent trials. Difference between the response time in correct congruent and incongruent trials is a rough measure of executive function, as it indicates the strength of the interference effect. [CITATION?]
WHAT MEASURES CAN YOU GAIN FROM THE FLANKER TASK?

    
A two-dimensional mental rotation task.
    Different mental rotation tasks have previously been used in game research, for different purposes. Boot et al. (2008) discovered that trained players  were no faster but were more accurate than non-trained players in a mental rotation task, and that playing Tetris led to the greatest improvement. Okagaki and Frensch (1994) also used Tetris, but discovered an improvement in mental rotation time, rather than accuracy. In their study on older adults, Maillot et al. (2012) discovered no improvement of game training on their visuospatial measures (including the mental rotation task). Guha, Jereen, DeGutis and Wilmer (2014) also did not find an training effect for performance in a mental rotation task from action video game play, whereas Feng et al. (2007) reported that video game play did create a training effect and reduced gender differences in a mental rotation task.

    The mental rotation task used in this study had three types of two-dimensional stimuli. Stimulus A consisted of one long line, perpendicularly intersected by two short ones. Stimulus B and C were similar to the stimulus A, except that they had one and two diagonal lines added to them, respectively. During the task, the participants were presented with an unrotated version of the target stimulus, and a version of the stimulus that was either an identical or mirrored version of the target, and rotated 0, 60, 120, 180, 240 or 300 degrees. The two images were displayed side-by-side, and the target stimulus was randomly displayed on either left or right side. The participant was instructed to assess as quickly and accurately as possible, whether the rotated image was identical or a mirror image of the target, and to press Q if they were identical and P if mirrored.


A spatial span task for assessing spatial working memory

The spatial span task was programmed similar to that of Owen and Hampshire (n.d.) as a variation of the Corsi block tapping task (Corsi, 1972). The stimuli were displayed as flashing rectangles in a four-times-four matrix of elements. After the stimulus was generated, the participant had to click on the boxes in the same order in which they flashed. The first stimulus had a length of four blocks, trial difficulty was lowered after two failures on the same stimulus length and increased after successful completion. The task was concluded after four failures.

    Visuo-spatial working memory tasks have been used in game studies before, e.g. by Maillot, Perrot and Hartley (2012); Boot, Framer, Simons, Fabiani and Gratton (2008); and Okagaki and Frensch (1994).

    Okagaki and Frensch (1994) discovered that playing tetris improves reaction times in mental rotation and spatial visualization tasks. In their study on older adults, Maillot et al. (2012) found no improvement of video-game training on a spatial span task. In their visual short-term memory test, Boot et al. (2008) discovered that expert players far outperformed non-gamers in a spatial span measure, but there was no difference between longitudinal groups, so the training effect was not clear. On their working memory operation span test, as well as on the Corsi block-tapping test, Boot et al. (2008) found no difference between experts and novices, and on their spatial 2-back task, experts were faster but no more accurate, with longitudinal groups improving with practice. The jury is out on the characteristics of training effects on spatial span tasks, but as training effects are not the subject of this study, the spatial span task was included into the battery, as Boot et al.'s (2008) findings imply some importance of visuo-spatial working memory functions for gameplay. 


the Tower of London test.
WHAT DOES THE TOWER OF LONDON TEST MEASURE?
READ UP ON https://neuropsychological-assessment-tests.com/sites/default/files/Tower%20of%20London%20Manual.pdf

... from there:
some researchers use the number of excess moves to measure performance while others use a perfect solution 
score (the number or percentage of problems correctly solved using the minimum number of moves), the total cognition time (this variable represents the total time that the participant is not holding a bead after the first move is made), the initial think time 
(this represents the time from when the problem is first displayed to when the first move is made), or the total move time (the amount of time that the participant spends—per problem—holding onto a bead). Given that a wide range of dependent measures have 
been used to evaluate the various aspects of planning and problem solving  performance, one must use caution when interpreting the findings and making  inferences about the development and decline of problem solving ability. The  computerized SANZEN TOL computes the typical dependent variables used in studies  examining TOL performance. The results printout for the SANZEN TOL test gives the  number of excess moves for each problem, the initial think time before the initial move  is made for each problem, the total think time on each problem, and the time taken to successfully complete each problem. 


    Boot, Framer, Simons, Fabiani and Gratton (2008) used the London Tower test. Unfortunately, the authors did no comparison between expert and novice gamers, but only looked at their longitudinal training groups, all of which improved the same. I decided to use the Tower of London despite of of its low presence in prior literature, because gameplay in League of Legends often involves the planning and execution of abilities or behaviors in a meaningful sequence in a setting where speed and accuracy is important.

    The Tower of London task consists of twenty-two pairs of images. The first image, image A, displays a starting point with three pegs of different lengths and of three balls of different colors, and image B displays an ending point with a final layout. The participant was asked to assess the smallest amount of moves they could use to get from the layout in image A to the layout in image B, and to respond by pressing the corresponding key on the keyboard. The moves required for the trials varied between one and six.




Karle J. W., Watter S., Shedden J. M. (2010). Task switching in video game players: benefits of selective attention but not resistance to proactive interference. Acta Psychologica (Amst.) 134(1).

Green, C. S., & Bavelier, D. (2003). Action video-game modifies visual selective attention. Nature, 423, 534-537.

Corsi, P.M.(1972), Human memory and the medial temporal region of the brain, Unpublished doctoral dissertation. McGill University.

Eriksen, B. A., & Eriksen, C. W. (1974). Effects of noise leters upon the identification of a target letter in a nonsearch task. Perception & Psychophysics, 16, 143-149.

Hart, S. G. & Staveland, L. E. (1988). Development of NASA-TLX (Task Load Index): Results of empirical and theoretical research. In: Human Mental Workload (P. A. Hancock and N. Meshkati (Eds.)), 139-183. North-Holland: Elsevier Science. 

Fletcher, R. B. & Whitton, S. M. (2014). The Group Environment Questionnaire: A Multilevel Confirmatory Factor Analysis.
Small Group Research, 45(1), 68-88. doi: 10.1177/1046496413511121

de Leeuw, J.R. (2014). jsPsych: A JavaScript library for creating behavioral experiments in a Web browser. Behavior Research Methods. Advance online publication. doi:10.3758/s13428-014-0458-y

Hart, S. (2006). Nasa-Task Load Index (Nasa-TLX); 20 Years Later. Human Factors and Ergonomics Society Annual Meeting Proceedings, 50, 904-908.

Pöllänen, T. (2014). Test battery of JavaScript cognitive tasks written using jsPsych for my master's thesis. Github repository. Retrieved from https://github.com/tuuleh/masters-battery.

Owen, A. M. & Hampshire, A. (n.d.). Spatial span ladder [software for online web-based testing]. Cambridge Brain Sciences Inc, University of Western Ontario, Canada. Retrieved from http://www.cambridgebrainsciences.com/browse/memory/test/spatial-span-ladder. 

Feng, J., Spence, I. & Pratt, J. (2007). Playing an action video game reduces gender differences in spatial cognition. Psychological science, 18(10), 850-855.

Maillot, P., Perrot, A. & Hartley, A. (2012). Effects of interactive physical-activity video-game training on physical and cognitive function in older adults. Psychology and Aging, 27(3), 589-600.

Anderson, C. A., Bailey, K. & West, R. (2010). A negative association between video game experience and proactive cognitive control. Psychophysiology, 47, 34/42.

Dye, M.W.G., Green, C.S., & Bavelier, D. (2009). The development of attention skills in action video game players. Neuropsychologia, 47(8-9), 1780-1789. doi:10.1016/j.neuropsychologia.2009.02.002.

Boot, W. R., Framer, A. F., Simons, D. J., Fabiani, M. & Gratton, G. (2008). The effects of video game playing on attention, memory and executive control. Acta Psychologica, 129, 387-398.

Okagaki, L., & Frensch, P. A. (1994). Effects of video game playing on measures of spatial performance: Gender effects in late adolescence. Journal of Applied Developmental Psychology, 15(1), 33-58.

Stins, J. F., Polderman, T. J. C., Boomsma, D. I., & de Geus, E. J. C. (2007). Conditional accuracy in response interference tasks: Evidence from the Eriksen flanker task and the spatial conflict task. Advances in Cognitive Psychology 3 (3), 389–396.

Mueller, S. T., & Piper, B. J. (2014). The Psychology Experiment Building Language (PEBL) and PEBL Test Battery. Journal of neuroscience methods (222), 250–259.

Best, J. R. (2012). Exergaming immediately enhances children's executive function. Developmental Psychology, 48(5), 1501-1510.

Guha, A., Jereen, A., DeGutis, J. & Wilmer, J. (2014). No action video game training effects for multiple object tracking or mental rotation. Journal of Vision, 22(14).

Colzato L. S., van Leeuwen P. J. A., van den Wildenberg W. P. M., Hommel B. (2010). DOOM’d to switch: superior cognitive flexibility in players of first person shooter games. Frontiers of psychology, 1(8).

Green C. S., Bavelier D. (2007). Action video game experience alters the spatial resolution of attention. Psychological Science, 18(1), 88-94.

Carron, A. V., Widmeyer, W. N., & Brawley, L. R. (1985). The development of an instrument to assess cohesion in sport teams: The Group Environment Questionnaire. Journal of Sport Psychology, 7, 244-266.

N. Pobiedina, J. Neidhardt, M. d. C. Calatrava Moreno & H. Werthner (2013). Ranking factors of team success in Proc. of the 22nd international conference on World Wide Web companion. International World Wide Web conferences Steering Committee, 2013, pp. 1185-1194.

Drachen, A., Yancey, M., Maguire, J., Chu, D., Yuhui Wang, I., Mahlmann, T., Schubert, M. & Klabajan, D. (2014) 
Skill-Based Differences in Spatio-Temporal Team Behaviour in Defence of The Ancients 2 (DotA 2) (Inproceeding)
Proceedings of the IEEE Games, Entertainment, and Media (GEM) Conference 2014.