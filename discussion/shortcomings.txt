- unstandardised instruments
- can't compare accuracy of RT measures
- server in San Francisco
- questionable blinding
- very self-selective sample
- limited duration of individual tasks increases alpha error probability, but was necessary to keep participant number high

Due to the game:
- it was difficult to recruit from all the game regions
- decay: ratings vary also due to inactivity, not due to player performance; 
- rankings are only a correlate of the underlying performance rating
    - MMR
- sampling at season's end
    - more active pvp players
    - ratings might not be representative
    - limited time for sampling

Improvements: 
- more time for sampling
- optimize the instruments
- use the Riot API at https://developer.riotgames.com/api


A few factors limit the scope of this study and should be discussed as its possible shortcomings.
The instruments were unstandardised and not absolutely comparable to those used in previous research. Additionally, there is very little research on how JavaScript-based solutions compare to laboratory experiments. The experiment was hosted on a SSD Cloud Server by Digital Ocean, located in San Francisco, which could mean that the timing for presenting stimuli and logging reaction times was slightly more accurate for participants who were geographically closer to the server. The test battery was made relatively short, approximately half an hour for a full completion, to minimize participant drop-out, and so the training trials were also relatively short, and the results for the cognitive tasks might have varied from differential training effects between participants, who might have needed a few more trials to get acquainted with the task. Finally, the sample was entirely self-selected, which may be a problem with insufficient binding (Andrews and Murphy, 2006). If the participants presume that the study deals with player performance, players with better ratings might behave differently from players with lower ratings during the cognitive tasks, which might reflect in their scores. I did not administer a post-experiment survey to assess, whether the blinding had been adequate, and hence cannot control for the effect. Finally, although I have information about the operating systems and browsers through which the participants accessed the experiment, I know nothing about the hardware or system load they had during the experiment. Hence, it is impossible to exclude the possibility, that more skilled players fare better in both, League of Legends rankings and web experiments due to higher quality hardware.





Andrews G., Murphy K. (2006). “Does video-game playing improve executive function?” in Frontiers in Cognitive Sciences, ed. Vanchevsky M. A., editor. (New York, NY: Nova Science Publishers, Inc.), 145–161 //this one is good for discussing shortcomings! FROM http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3171788/