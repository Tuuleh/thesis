- instrument not 100% comparable to previous research
- unstandardised instruments
- only one game
- web-experiments (accuracy)
- server in San Francisco
- inadequate blinding, which might not be as severe as in VGP vs. NVGP studies, since everyone played video games and the population was hence more homogenic
- very self-selective sample
-'candy' between the experiments -> influence motivation (response was very positive)
- limited duration of individual tasks increases alpha error probability, but was necessary to keep participant number high

Due to the game:
- it was difficult to recruit from all the game regions
- decay: ratings vary also due to inactivity, not due to player performance; MMR: separate match maker rating for solo and team play
- the season was just about to come into end -> this could have been a good or a bad thing:
    - more active pvp players, but ratings might not be representative of the team's typical rating throughout the season
    - limited time for sampling


A few factors limit the scope of this study and should be discussed as its possible shortcomings.
The instruments were unstandardised and not absolutely comparable to those used in previous research. Additionally, there is very little research on how JavaScript-based solutions compare to laboratory experiments. The experiment was hosted on a SSD Cloud Server by Digital Ocean, located in San Francisco, which could mean that the timing for presenting stimuli and logging reaction times was slightly more accurate for participants who were geographically closer to the server. The test battery was made relatively short, approximately half an hour for a full completion, to minimize participant drop-out, and so the training trials were also relatively short, and the results for the cognitive tasks might have varied from differential training effects between participants, who might have needed a few more trials to get acquainted with the task. Finally, the sample was entirely self-selected, which may be a problem with insufficient binding (Andrews and Murphy, 2006). If the participants presume that the study deals with player performance, players with better ratings might behave differently from players with lower ratings during the cognitive tasks, which might reflect in their scores. I did not administer a post-experiment survey to assess, whether the blinding had been adequate, and hence cannot control for the effect. Finally, although I have information about the operating systems and browsers through which the participants accessed the experiment, I know nothing about the hardware or system load they had during the experiment. Hence, it is impossible to exclude the possibility, that more skilled players fare better in both, League of Legends rankings and web experiments due to higher quality hardware.


Improvements: 
- better sampling, 
- use the Riot API at https://developer.riotgames.com/api - problem: request limit (ten requests per ten seconds)


Andrews G., Murphy K. (2006). “Does video-game playing improve executive function?” in Frontiers in Cognitive Sciences, ed. Vanchevsky M. A., editor. (New York, NY: Nova Science Publishers, Inc.), 145–161 //this one is good for discussing shortcomings! FROM http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3171788/